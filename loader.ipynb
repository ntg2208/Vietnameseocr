{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"loader.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"YmyGW5aRJB7F","colab_type":"code","outputId":"e359ec82-d056-45f5-eda4-b8dcfbfdb60c","executionInfo":{"status":"ok","timestamp":1554532773854,"user_tz":-420,"elapsed":1043,"user":{"displayName":"GIANG NGUYỄN TRƯỜNG","photoUrl":"","userId":"13515963877684298020"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","import os\n","\n","os.chdir(\"gdrive/My Drive/Playground\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"QF5ZPqpPJKRC","colab_type":"code","outputId":"d3b13b55-3cc6-4d26-99d4-c49cf7366720","executionInfo":{"status":"ok","timestamp":1554532775455,"user_tz":-420,"elapsed":2639,"user":{"displayName":"GIANG NGUYỄN TRƯỜNG","photoUrl":"","userId":"13515963877684298020"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import json\n","import cv2\n","import os, random\n","import numpy as np\n","import tensorflow as tf\n","import keras\n","from keras.preprocessing import image\n","from keras.applications.vgg16 import preprocess_input\n","from keras import backend as K\n","from keras.layers import multiply, Dense, Permute, Lambda, RepeatVector\n","import itertools\n","import editdistance\n","from lib.random_eraser import get_random_eraser\n","from keras.preprocessing.image import ImageDataGenerator\n","from scipy.ndimage.interpolation import map_coordinates\n","from scipy.ndimage.filters import gaussian_filter\n","from random import randint\n","from PIL import Image\n","import json\n","random.seed(2018)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"Wc24sd73JRrA","colab_type":"code","colab":{}},"cell_type":"code","source":["letters = \" !\\\"#&\\\\'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzÂÊÔàáâãèéêìíòóôõùúýăĐđĩũƠơưạảấầẩậắằẵặẻẽếềểễệỉịọỏốồổỗộớờởỡợụủỨứừửữựỳỵỷỹ\"\n","MAX_LEN = 70\n","WIDTH, HEIGHT = 1280, 64\n","SIZE = WIDTH, HEIGHT\n","CHAR_DICT = len(letters) + 1\n","\n","chars = letters\n","wordChars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzÂÊÔàáâãèéêìíòóôõùúýăĐđĩũƠơưạảấầẩậắằẵặẻẽếềểễệỉịọỏốồổỗộớờởỡợụủỨứừửữựỳỵỷỹ\"\n","corpus = ' \\n '.join(json.load(open('labels.json')).values())\n","word_beam_search_module = tf.load_op_library('lib/xxxx.so')\n","mat=tf.placeholder(tf.float32)\n","beamsearch_decoder = word_beam_search_module.word_beam_search(mat, 25, 'Words', 0.1, corpus, chars, wordChars)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"39L2qsZJJWOf","colab_type":"code","outputId":"48828168-c9b8-472c-c68b-6bf9fba3a34b","executionInfo":{"status":"ok","timestamp":1554535144782,"user_tz":-420,"elapsed":794,"user":{"displayName":"GIANG NGUYỄN TRƯỜNG","photoUrl":"","userId":"13515963877684298020"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["img = image.load_img(self.img_dirpath + img_file, target_size=SIZE[::-1], interpolation='bicubic')"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"]},"metadata":{"tags":[]},"execution_count":2}]},{"metadata":{"id":"RE3SbJIKJgu1","colab_type":"code","colab":{}},"cell_type":"code","source":["def text_to_labels(text):\n","    return list(map(lambda x: letters.index(x), text))\n","\n","def labels_to_text(labels):\n","    return ''.join(list(map(lambda x: letters[x] if x < len(letters) else \"\", labels)))\n","\n","\n","def beamsearch(sess, y_pred):\n","    y_pred = y_pred.transpose((1, 0, 2))\n","    results = sess.run(beamsearch_decoder, {mat:y_pred[2:]})\n","    blank=len(chars)\n","    results_text = []\n","    for res in results:\n","        s=''\n","        for label in res:\n","            if label==blank:\n","                break\n","            s+=chars[label] # map label to char\n","        results_text.append(s)\n","    return results_text\n","\n","def ctc_lambda_func(args):\n","    y_pred, labels, input_length, label_length = args\n","    # the 2 is critical here since the first couple outputs of the RNN\n","    # tend to be garbage:\n","    y_pred = y_pred[:, 2:, :]\n","    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n","\n","def attention_rnn(inputs):\n","    # inputs.shape = (batch_size, time_steps, input_dim)\n","    input_dim = int(inputs.shape[2])\n","    timestep = int(inputs.shape[1])\n","    a = Permute((2, 1))(inputs)\n","    a = Dense(timestep, activation='softmax')(a)\n","    a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n","    a = RepeatVector(input_dim)(a)\n","    a_probs = Permute((2, 1), name='attention_vec')(a)\n","    output_attention_mul = multiply([inputs, a_probs], name='attention_mul')\n","    return output_attention_mul\n","\n","def decode_batch(out):\n","    ret = []\n","    for j in range(out.shape[0]):\n","        out_best = list(np.argmax(out[j, 2:], 1))\n","        out_best = [k for k, g in itertools.groupby(out_best)]\n","        outstr = labels_to_text(out_best)\n","        ret.append(outstr)\n","    return ret\n","\n","# Function to distort image\n","def elastic_transform(image, alpha, sigma, alpha_affine, random_state=None):\n","    \"\"\"Elastic deformation of images as described in [Simard2003]_ (with modifications).\n","    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n","         Convolutional Neural Networks applied to Visual Document Analysis\", in\n","         Proc. of the International Conference on Document Analysis and\n","         Recognition, 2003.\n","\n","     Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5\n","    \"\"\"\n","    if random_state is None:\n","        random_state = np.random.RandomState(None)\n","\n","    shape = image.shape\n","    shape_size = shape[:2]\n","    \n","    # Random affine\n","    center_square = np.float32(shape_size) // 2\n","    square_size = min(shape_size) // 3\n","    pts1 = np.float32([center_square + square_size, [center_square[0]+square_size, center_square[1]-square_size], center_square - square_size])\n","    pts2 = pts1 + random_state.uniform(-alpha_affine, alpha_affine, size=pts1.shape).astype(np.float32)\n","    M = cv2.getAffineTransform(pts1, pts2)\n","    image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n","\n","    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n","    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n","    dz = np.zeros_like(dx)\n","\n","    x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n","    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1)), np.reshape(z, (-1, 1))\n","\n","    return map_coordinates(image, indices, order=1, mode='reflect').reshape(shape)\n","\n","class VizCallback(keras.callbacks.Callback):\n","    def __init__(self, sess, y_func, text_img_gen, text_size, num_display_words=3):\n","        self.y_func = y_func\n","        self.text_img_gen = text_img_gen\n","        self.num_display_words = num_display_words\n","        self.text_size = text_size\n","        self.sess = sess\n","\n","    def show_edit_distance(self, num):\n","        num_left = num\n","        mean_norm_ed = 0.0\n","        mean_ed = 0.0\n","        while num_left > 0:\n","            word_batch = next(self.text_img_gen.next_batch())[0]\n","            num_proc = min(word_batch['the_inputs'].shape[0], num_left)\n","            # predict\n","            inputs = word_batch['the_inputs'][0:num_proc]\n","            pred = self.y_func([inputs])[0]\n","            decoded_res = beamsearch(self.sess, pred)#decode_batch(pred)\n","            # label\n","            labels = word_batch['the_labels'][:num_proc].astype(np.int32)\n","            labels = [labels_to_text(label) for label in labels]\n","            \n","            for j in range(num_proc):\n","                edit_dist = editdistance.eval(decoded_res[j], labels[j])\n","                mean_ed += float(edit_dist)\n","                mean_norm_ed += float(edit_dist) / len(labels[j])\n","\n","            num_left -= num_proc\n","        mean_norm_ed = mean_norm_ed / num\n","        mean_ed = mean_ed / num\n","        print('\\nOut of %d samples:  Mean edit distance: '\n","              '%.3f Mean normalized edit distance: %0.3f'\n","              % (num, mean_ed, mean_norm_ed))\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        batch = next(self.text_img_gen.next_batch())[0]\n","        inputs = batch['the_inputs'][:self.num_display_words]\n","        labels = batch['the_labels'][:self.num_display_words].astype(np.int32)\n","        labels = [labels_to_text(label) for label in labels]\n","         \n","        pred = self.y_func([inputs])[0]\n","        pred_beamsearch_texts = beamsearch(self.sess, pred)\n","        #pred_texts = decode_batch(pred)\n","        for i in range(min(self.num_display_words, len(inputs))):\n","            print(\"label: {} - predict: {}\".format(labels[i], pred_beamsearch_texts[i]))\n","\n","        self.show_edit_distance(self.text_size)\n","\n","class TextImageGenerator:\n","    def __init__(self, img_dirpath, labels_path, img_w, img_h,\n","                 batch_size, downsample_factor, idxs, training=True, max_text_len=9, n_eraser=5):\n","        self.img_h = img_h\n","        self.img_w = img_w\n","        self.batch_size = batch_size\n","        self.max_text_len = max_text_len\n","        self.idxs = idxs\n","        self.downsample_factor = downsample_factor\n","        self.img_dirpath = img_dirpath                  # image dir path\n","        self.labels= json.load(open(labels_path)) if labels_path != None else None\n","        self.img_dir = sorted(os.listdir(self.img_dirpath))     # images list\n","        random.shuffle(self.img_dir)\n","\n","        if self.idxs is not None:\n","            self.img_dir = [self.img_dir[idx] for idx in self.idxs]\n","\n","        self.n = len(self.img_dir)                      # number of images\n","        self.indexes = list(range(self.n))\n","        self.cur_index = 0\n","        self.imgs = np.ones((self.n, self.img_h, self.img_w, 3), dtype=np.float16)\n","        self.training = training\n","        self.n_eraser = n_eraser\n","        self.random_eraser = get_random_eraser(s_l=0.004, s_h=0.005, r_1=0.01, r_2=1/0.01, v_l=-128, v_h=128)\n","        self.texts = []\n","        image_datagen_args = {\n","\t\t'shear_range': 0.1,\n","\t\t'zoom_range': 0.01,\n","\t\t'width_shift_range': 0.001,\n","\t\t'height_shift_range': 0.1,\n","\t\t'rotation_range': 1,\n","\t\t'horizontal_flip': False,\n","\t\t'vertical_flip': False\n","\t}\n","        self.image_datagen = ImageDataGenerator(**image_datagen_args)\n","\n","    def build_data(self):\n","        print(self.n, \" Image Loading start... \", self.img_dirpath)\n","        for i, img_file in enumerate(self.img_dir):\n","            img = image.load_img(self.img_dirpath + img_file, target_size=SIZE[::-1], interpolation='bicubic')\n","            img = image.img_to_array(img)\n","            img = preprocess_input(img)\n","            self.imgs[i] = img\n","            if self.labels != None: \n","                self.texts.append(self.labels[img_file][:MAX_LEN])\n","            else:\n","                #valid mode\n","                self.texts.append('')\n","        print(\"Image Loading finish...\")\n","\n","    def next_sample(self):\n","        self.cur_index += 1\n","        if self.cur_index >= self.n:\n","            self.cur_index = 0\n","            random.shuffle(self.indexes)\n","        return self.imgs[self.indexes[self.cur_index]].astype(np.float32), self.texts[self.indexes[self.cur_index]]\n","\n","    def next_batch(self):\n","        while True:\n","            X_data = np.zeros([self.batch_size, self.img_w, self.img_h, 3], dtype=np.float32)     # (bs, 128, 64, 1)\n","            Y_data = np.zeros([self.batch_size, self.max_text_len], dtype=np.float32)             # (bs, 9)\n","            input_length = np.ones((self.batch_size, 1), dtype=np.float32) * (self.img_w // self.downsample_factor - 2)  # (bs, 1)\n","            label_length = np.zeros((self.batch_size, 1), dtype=np.float32)           # (bs, 1)\n","\n","            for i in range(self.batch_size):\n","                img, text = self.next_sample()\n","\n","                if self.training:\n","                    params = self.image_datagen.get_random_transform(img.shape)\n","                    img = self.image_datagen.apply_transform(img, params)\n","                    if randint(0, 1) == 1:\n","                        for _ in range(self.n_eraser):\n","                            img = self.random_eraser(img)\n","                        img = elastic_transform(img, 10, 2, 0.1)\n","\n","                img = img.transpose((1, 0, 2))\n","                # random eraser if training\n","                X_data[i] = img\n","                Y_data[i,:len(text)] = text_to_labels(text)\n","                label_length[i] = len(text)\n","\n","            inputs = {\n","                'the_inputs': X_data,  # (bs, 128, 64, 1)\n","                'the_labels': Y_data,  # (bs, 8)\n","                'input_length': input_length,  # (bs, 1)\n","                'label_length': label_length  # (bs, 1)\n","            }\n","\n","            outputs = {'ctc': np.zeros([self.batch_size])}   # (bs, 1)\n","            yield (inputs, outputs)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vUipEqkfJkk7","colab_type":"code","colab":{}},"cell_type":"code","source":["letters = \" aAáÁàÀảẢãÃạẠăĂắẮằẰẳẲẵẴặẶâÂấẤầẦẩẨẫẪậẬbBcCdDđĐgGhHeEéÉèÈẻẺẽẼẹẸêÊếẾềỀểỂễỄệỆiIíÍìÌỉỈĩĨịỊ!@#$%^&*kKlLmMnNpPqQoOóÓòÒỏỎõÕọỌôÔốỐồỒổỔỗỖộỘơƠớỚờỜởỞỡỠợỢrRsStTvVxXyYwWuUúÚùÙủỦũŨ()+=;‘’ưƯứỨừỪửỬựỰ“”\\|?0123456789-_:/.<>,\"\n","print(len(letters))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DVxA78bU3ezq","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}